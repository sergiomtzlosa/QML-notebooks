{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test ANN - Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Flatten, Dense, Input, Concatenate, concatenate, BatchNormalization, Activation\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=[28, 28]))\n",
    "model.add(Dense(300, activation=\"relu\"))\n",
    "model.add(Dense(100, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.reshaping.flatten.Flatten at 0x1ea618be948>,\n",
       " <keras.layers.core.dense.Dense at 0x1ea6176cf88>,\n",
       " <keras.layers.core.dense.Dense at 0x1ea5e8675c8>,\n",
       " <keras.layers.core.dense.Dense at 0x1ea61aa57c8>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      "55000/55000 [==============================] - 2s 45us/sample - loss: 0.7252 - acc: 0.7613 - val_loss: 0.5213 - val_acc: 0.8204\n",
      "Epoch 2/30\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.4902 - acc: 0.8292 - val_loss: 0.4848 - val_acc: 0.8258\n",
      "Epoch 3/30\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.4437 - acc: 0.8445 - val_loss: 0.4147 - val_acc: 0.8612\n",
      "Epoch 4/30\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.4162 - acc: 0.8535 - val_loss: 0.3986 - val_acc: 0.8608\n",
      "Epoch 5/30\n",
      "55000/55000 [==============================] - 2s 40us/sample - loss: 0.3941 - acc: 0.8620 - val_loss: 0.3780 - val_acc: 0.8718\n",
      "Epoch 6/30\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.3784 - acc: 0.8677 - val_loss: 0.3794 - val_acc: 0.8692\n",
      "Epoch 7/30\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.3646 - acc: 0.8706 - val_loss: 0.3544 - val_acc: 0.8792\n",
      "Epoch 8/30\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.3530 - acc: 0.8750 - val_loss: 0.3551 - val_acc: 0.8736\n",
      "Epoch 9/30\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.3419 - acc: 0.8785 - val_loss: 0.3611 - val_acc: 0.8728\n",
      "Epoch 10/30\n",
      "55000/55000 [==============================] - 2s 38us/sample - loss: 0.3320 - acc: 0.8827 - val_loss: 0.3379 - val_acc: 0.8824\n",
      "Epoch 11/30\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.3242 - acc: 0.8841 - val_loss: 0.3389 - val_acc: 0.8826\n",
      "Epoch 12/30\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.3157 - acc: 0.8873 - val_loss: 0.3324 - val_acc: 0.8844\n",
      "Epoch 13/30\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.3090 - acc: 0.8890 - val_loss: 0.3413 - val_acc: 0.8834\n",
      "Epoch 14/30\n",
      "55000/55000 [==============================] - 2s 41us/sample - loss: 0.3024 - acc: 0.8921 - val_loss: 0.3301 - val_acc: 0.8810\n",
      "Epoch 15/30\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.2952 - acc: 0.8941 - val_loss: 0.3465 - val_acc: 0.8776\n",
      "Epoch 16/30\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.2890 - acc: 0.8961 - val_loss: 0.3282 - val_acc: 0.8844\n",
      "Epoch 17/30\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.2833 - acc: 0.8978 - val_loss: 0.3124 - val_acc: 0.8884\n",
      "Epoch 18/30\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 0.2786 - acc: 0.9003 - val_loss: 0.3114 - val_acc: 0.8896\n",
      "Epoch 19/30\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 0.2733 - acc: 0.8999 - val_loss: 0.3150 - val_acc: 0.8876\n",
      "Epoch 20/30\n",
      "55000/55000 [==============================] - 2s 39us/sample - loss: 0.2670 - acc: 0.9031 - val_loss: 0.3208 - val_acc: 0.8856\n",
      "Epoch 21/30\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.2626 - acc: 0.9062 - val_loss: 0.3121 - val_acc: 0.8884\n",
      "Epoch 22/30\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.2575 - acc: 0.9061 - val_loss: 0.3077 - val_acc: 0.8886\n",
      "Epoch 23/30\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.2545 - acc: 0.9075 - val_loss: 0.3097 - val_acc: 0.8898\n",
      "Epoch 24/30\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.2491 - acc: 0.9109 - val_loss: 0.3220 - val_acc: 0.8856\n",
      "Epoch 25/30\n",
      "55000/55000 [==============================] - 2s 44us/sample - loss: 0.2450 - acc: 0.9113 - val_loss: 0.3086 - val_acc: 0.8928\n",
      "Epoch 26/30\n",
      "55000/55000 [==============================] - 2s 41us/sample - loss: 0.2409 - acc: 0.9130 - val_loss: 0.3057 - val_acc: 0.8926\n",
      "Epoch 27/30\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.2370 - acc: 0.9131 - val_loss: 0.3114 - val_acc: 0.8872\n",
      "Epoch 28/30\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.2331 - acc: 0.9161 - val_loss: 0.3006 - val_acc: 0.8940\n",
      "Epoch 29/30\n",
      "55000/55000 [==============================] - 2s 43us/sample - loss: 0.2297 - acc: 0.9174 - val_loss: 0.2957 - val_acc: 0.8940\n",
      "Epoch 30/30\n",
      "55000/55000 [==============================] - 2s 42us/sample - loss: 0.2255 - acc: 0.9187 - val_loss: 0.2939 - val_acc: 0.8964\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62.60356554617882, 0.8528]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Regression MLP Using the Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.32520000e+00,  4.10000000e+01,  6.98412698e+00,\n",
       "         1.02380952e+00,  3.22000000e+02,  2.55555556e+00,\n",
       "         3.78800000e+01, -1.22230000e+02],\n",
       "       [ 8.30140000e+00,  2.10000000e+01,  6.23813708e+00,\n",
       "         9.71880492e-01,  2.40100000e+03,  2.10984183e+00,\n",
       "         3.78600000e+01, -1.22220000e+02],\n",
       "       [ 7.25740000e+00,  5.20000000e+01,  8.28813559e+00,\n",
       "         1.07344633e+00,  4.96000000e+02,  2.80225989e+00,\n",
       "         3.78500000e+01, -1.22240000e+02],\n",
       "       [ 5.64310000e+00,  5.20000000e+01,  5.81735160e+00,\n",
       "         1.07305936e+00,  5.58000000e+02,  2.54794521e+00,\n",
       "         3.78500000e+01, -1.22250000e+02],\n",
       "       [ 3.84620000e+00,  5.20000000e+01,  6.28185328e+00,\n",
       "         1.08108108e+00,  5.65000000e+02,  2.18146718e+00,\n",
       "         3.78500000e+01, -1.22250000e+02]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "X_new = X_test[:3] # pretend these are new instances\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input = Input(shape=X_train.shape[1:])\n",
    "#hidden1 = Dense(30, activation=\"relu\")(input)\n",
    "#hidden2 = Dense(30, activation=\"relu\")(hidden1)\n",
    "#concat = Concatenate()([input, hidden2])\n",
    "#output = Dense(1)(concat)\n",
    "\n",
    "#model = Model(inputs=[input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = Input(shape=[5])\n",
    "input_B = Input(shape=[6])\n",
    "hidden1 = Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = concatenate([input_A, hidden2])\n",
    "output = Dense(1)(concat)\n",
    "model = Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"sgd\")\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/20\n",
      "11610/11610 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "11610/11610 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "11610/11610 [==============================] - 0s 25us/sample - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,validation_data=((X_valid_A, X_valid_B), y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    \n",
    "    model = Sequential()\n",
    "    options = {\"input_shape\": input_shape}\n",
    "    \n",
    "    for layer in range(n_hidden):\n",
    "        model.add(Dense(n_neurons, activation=\"relu\", **options))\n",
    "        options = {}\n",
    "    \n",
    "    model.add(Dense(1, **options))\n",
    "    \n",
    "    optimizer = SGD(learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs=100, validation_data = (X_valid, y_valid), callbacks=[EarlyStopping(patience=10)])\n",
    "\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "\n",
    "y_pred = keras_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 30us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 39us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 26us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: 673129206660921425920.0000 - val_loss: 377907368880488576.0000\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 203053450083455232.0000 - val_loss: 92052453507203072.0000\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 49460718752327096.0000 - val_loss: 22422581143404544.0000\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 12047878185626098.0000 - val_loss: 5461802531979010.0000\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 2934680274674040.5000 - val_loss: 1330410185384863.2500\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 714843062948697.3750 - val_loss: 324067765564262.0000\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 174124765592541.5938 - val_loss: 78937842577202.6719\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 42414063934162.8828 - val_loss: 19228019700446.5273\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 10331410718275.4727 - val_loss: 4683650539575.0371\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 26us/sample - loss: 2516573009961.2773 - val_loss: 1140866062576.2563\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 612998971679.8843 - val_loss: 277897612975.4294\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 149317179659.7747 - val_loss: 67691660225.0253\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 36371406606.1561 - val_loss: 16488646286.6191\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 8859518355.3819 - val_loss: 4016382569.5752\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 2158043430.9953 - val_loss: 978329344.2315\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 525666453.8543 - val_loss: 238305970.9933\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 128044342.1313 - val_loss: 58047664.2749\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 31189637.2517 - val_loss: 14139520.4982\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 7597326.1787 - val_loss: 3444165.3293\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 1850593.1921 - val_loss: 838940.8555\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 450776.3449 - val_loss: 204352.2295\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 109802.9700 - val_loss: 49776.6340\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 26747.2034 - val_loss: 12125.3351\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: 6516.1712 - val_loss: 2954.2958\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1588.2123 - val_loss: 720.5174\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 387.8597 - val_loss: 176.4224\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 95.4458 - val_loss: 43.9524\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 24.2434 - val_loss: 11.7081\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 6.9064 - val_loss: 3.8709\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 2.6815 - val_loss: 1.9598\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.6509 - val_loss: 1.4965\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.4006 - val_loss: 1.3850\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3395 - val_loss: 1.3580\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3246 - val_loss: 1.3518\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3210 - val_loss: 1.3503\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3201 - val_loss: 1.3501\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3200 - val_loss: 1.3501\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3199 - val_loss: 1.3501\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 1.3199 - val_loss: 1.3501\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 1.3199 - val_loss: 1.3501\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3199 - val_loss: 1.3501\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: 1.3199 - val_loss: 1.3501\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3199 - val_loss: 1.3501\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 1.3199 - val_loss: 1.3501\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3199 - val_loss: 1.3501\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3199 - val_loss: 1.3501\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3199 - val_loss: 1.3501\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 40us/sample - loss: 2281698293213406758235537408.0000 - val_loss: 1076313978467168154550272.0000\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 578314070177465294127104.0000 - val_loss: 262173161336941876084736.0000\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: 140868340980140273041408.0000 - val_loss: 63861457117102601142272.0000\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: 34313394098799833513984.0000 - val_loss: 15555674055517757505536.0000\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 26us/sample - loss: 8358218160774090588160.0000 - val_loss: 3789123814868466532352.0000\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 2035933470432940785664.0000 - val_loss: 922971507545495044096.0000\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 495921972010327998464.0000 - val_loss: 224821874829404667904.0000\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 120799077327334211584.0000 - val_loss: 54763199756872171520.0000\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 29424817613262127104.0000 - val_loss: 13339478486352916480.0000\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 7167439157207111680.0000 - val_loss: 3249294079711772672.0000\n",
      "Epoch 11/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1745877832070135296.0000 - val_loss: 791477810790170368.0000\n",
      "Epoch 12/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 425269259106759168.0000 - val_loss: 192792165698510784.0000\n",
      "Epoch 13/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 103589131899962880.0000 - val_loss: 46961172414464000.0000\n",
      "Epoch 14/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 25232706590806288.0000 - val_loss: 11439024778445308.0000\n",
      "Epoch 15/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 6146303201889857.0000 - val_loss: 2786373455052800.0000\n",
      "Epoch 16/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1497145862922379.7500 - val_loss: 678718247303065.3750\n",
      "Epoch 17/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 364682079961257.3125 - val_loss: 165325262950165.0312\n",
      "Epoch 18/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 88830921350122.3125 - val_loss: 40270738895226.9062\n",
      "Epoch 19/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 21637868859178.2031 - val_loss: 9809341525683.6621\n",
      "Epoch 20/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 5270656446732.8330 - val_loss: 2389406650742.6729\n",
      "Epoch 21/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1283851663604.4897 - val_loss: 582022423353.5504\n",
      "Epoch 22/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 312726498360.6243 - val_loss: 141771857941.1680\n",
      "Epoch 23/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 76175529989.2920 - val_loss: 34533505918.3463\n",
      "Epoch 24/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 18555197595.5845 - val_loss: 8411823028.0599\n",
      "Epoch 25/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 4519758623.2889 - val_loss: 2048991551.9669\n",
      "Epoch 26/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1100944367.2310 - val_loss: 499103465.4429\n",
      "Epoch 27/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 268173365.3788 - val_loss: 121574048.1902\n",
      "Epoch 28/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 65322997.2186 - val_loss: 29613576.1674\n",
      "Epoch 29/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 15911685.2140 - val_loss: 7213408.6605\n",
      "Epoch 30/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 3875842.8760 - val_loss: 1757073.6939\n",
      "Epoch 31/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 944097.8555 - val_loss: 427997.1750\n",
      "Epoch 32/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 229968.9249 - val_loss: 104253.0640\n",
      "Epoch 33/100\n",
      "7740/7740 [==============================] - 0s 29us/sample - loss: 56017.8774 - val_loss: 25395.1549\n",
      "Epoch 34/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 13646.2169 - val_loss: 6186.9534\n",
      "Epoch 35/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 3325.0382 - val_loss: 1507.8726\n",
      "Epoch 36/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 810.9322 - val_loss: 368.3304\n",
      "Epoch 37/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 198.5086 - val_loss: 90.6745\n",
      "Epoch 38/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 49.3412 - val_loss: 23.0934\n",
      "Epoch 39/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 13.0204 - val_loss: 6.6461\n",
      "Epoch 40/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 4.1769 - val_loss: 2.6397\n",
      "Epoch 41/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 2.0206 - val_loss: 1.6642\n",
      "Epoch 42/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.4942 - val_loss: 1.4252\n",
      "Epoch 43/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3648 - val_loss: 1.3675\n",
      "Epoch 44/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3335 - val_loss: 1.3533\n",
      "Epoch 45/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 1.3255 - val_loss: 1.3498\n",
      "Epoch 46/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 1.3236 - val_loss: 1.3491\n",
      "Epoch 47/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 1.3232 - val_loss: 1.3490\n",
      "Epoch 48/100\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: 1.3231 - val_loss: 1.3489\n",
      "Epoch 49/100\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: 1.3231 - val_loss: 1.3489\n",
      "Epoch 50/100\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: 1.3231 - val_loss: 1.3489\n",
      "Epoch 51/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3230 - val_loss: 1.3490\n",
      "Epoch 52/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3231 - val_loss: 1.3490\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3231 - val_loss: 1.3490\n",
      "Epoch 54/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3231 - val_loss: 1.3490\n",
      "Epoch 55/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3231 - val_loss: 1.3489\n",
      "Epoch 56/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3231 - val_loss: 1.3489\n",
      "Epoch 57/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: 1.3231 - val_loss: 1.3489\n",
      "Epoch 58/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: 1.3231 - val_loss: 1.3490\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 50us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 30us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 29us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 26us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 30us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 30us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 26us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 26us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 26us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 33us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 29us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 26us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 48us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 53us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 26us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 30us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 26us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 30us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 19us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 60us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 51us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 52us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 65us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 33us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 67us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 33us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 33us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 74us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 33us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 35us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 40us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7740/7740 [==============================] - 0s 41us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 79us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 26us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 63us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 26us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 28us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 0s 64us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 69us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 20us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 21us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 27us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 25us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 67us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 83us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 30us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 38us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 80us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 30us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Train on 7740 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "7740/7740 [==============================] - 1s 81us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "7740/7740 [==============================] - 0s 37us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "7740/7740 [==============================] - 0s 34us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "7740/7740 [==============================] - 0s 32us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "7740/7740 [==============================] - 0s 31us/sample - loss: nan - val_loss: nan\n",
      "Train on 11610 samples, validate on 3870 samples\n",
      "Epoch 1/100\n",
      "11610/11610 [==============================] - 1s 56us/sample - loss: nan - val_loss: nan\n",
      "Epoch 2/100\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 3/100\n",
      "11610/11610 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 4/100\n",
      "11610/11610 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 5/100\n",
      "11610/11610 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 6/100\n",
      "11610/11610 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 7/100\n",
      "11610/11610 [==============================] - 0s 22us/sample - loss: nan - val_loss: nan\n",
      "Epoch 8/100\n",
      "11610/11610 [==============================] - 0s 23us/sample - loss: nan - val_loss: nan\n",
      "Epoch 9/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n",
      "Epoch 10/100\n",
      "11610/11610 [==============================] - 0s 24us/sample - loss: nan - val_loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x000001EA085E7D08>,\n",
       "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001EA0863F888>,\n",
       "                                        'n_hidden': [0, 1, 2, 3],\n",
       "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
       "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
       "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
       "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3)\n",
    "\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.011727593349882064, 'n_hidden': 0, 'n_neurons': 89}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Batch Normalization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=[28, 28]),\n",
    "    BatchNormalization(),\n",
    "    Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    BatchNormalization(),\n",
    "    Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_2 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Operation 'batch_normalization/cond_2/Merge' type=Merge>,\n",
       " <tf.Operation 'batch_normalization/cond_3/Merge' type=Merge>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Flatten(input_shape=[28, 28]),\n",
    "    BatchNormalization(),\n",
    "    Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    BatchNormalization(),\n",
    "    Activation(\"elu\"),\n",
    "    Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
    "    Activation(\"elu\"),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 784)              3136      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               235200    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               30000     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "#model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "#model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_A_clone = keras.models.clone_model(model_A)\n",
    "#model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for layer in model_B_on_A.layers[:-1]:\n",
    "#    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile your model after you freeze or unfreeze layers\n",
    "#model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
