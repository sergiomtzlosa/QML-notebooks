{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad2fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 7]\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571cb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/household_power_consumption.txt', delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e12c70b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Global_active_power</th>\n",
       "      <th>Global_reactive_power</th>\n",
       "      <th>Voltage</th>\n",
       "      <th>Global_intensity</th>\n",
       "      <th>Sub_metering_1</th>\n",
       "      <th>Sub_metering_2</th>\n",
       "      <th>Sub_metering_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:24:00</td>\n",
       "      <td>4.216</td>\n",
       "      <td>0.418</td>\n",
       "      <td>234.840</td>\n",
       "      <td>18.400</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:25:00</td>\n",
       "      <td>5.360</td>\n",
       "      <td>0.436</td>\n",
       "      <td>233.630</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:26:00</td>\n",
       "      <td>5.374</td>\n",
       "      <td>0.498</td>\n",
       "      <td>233.290</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:27:00</td>\n",
       "      <td>5.388</td>\n",
       "      <td>0.502</td>\n",
       "      <td>233.740</td>\n",
       "      <td>23.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16/12/2006</td>\n",
       "      <td>17:28:00</td>\n",
       "      <td>3.666</td>\n",
       "      <td>0.528</td>\n",
       "      <td>235.680</td>\n",
       "      <td>15.800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Time Global_active_power Global_reactive_power  Voltage  \\\n",
       "0  16/12/2006  17:24:00               4.216                 0.418  234.840   \n",
       "1  16/12/2006  17:25:00               5.360                 0.436  233.630   \n",
       "2  16/12/2006  17:26:00               5.374                 0.498  233.290   \n",
       "3  16/12/2006  17:27:00               5.388                 0.502  233.740   \n",
       "4  16/12/2006  17:28:00               3.666                 0.528  235.680   \n",
       "\n",
       "  Global_intensity Sub_metering_1 Sub_metering_2  Sub_metering_3  \n",
       "0           18.400          0.000          1.000            17.0  \n",
       "1           23.000          0.000          1.000            16.0  \n",
       "2           23.000          0.000          2.000            17.0  \n",
       "3           23.000          0.000          1.000            17.0  \n",
       "4           15.800          0.000          1.000            17.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0049b41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows and columns after removing missing values: (2049280, 2)\n",
      "The time series starts from:  2006-12-16 17:24:00\n",
      "The time series ends on:  2010-12-11 23:59:00\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This code is copied from https://towardsdatascience.com/time-series-analysis-visualization-forecasting-with-lstm-77a905180eba\n",
    "# with a few minor changes.\n",
    "#\n",
    "df['date_time'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])\n",
    "df['Global_active_power'] = pd.to_numeric(df['Global_active_power'], errors='coerce')\n",
    "df = df.dropna(subset=['Global_active_power'])\n",
    "\n",
    "df['date_time'] = pd.to_datetime(df['date_time'])\n",
    "\n",
    "df = df.loc[:, ['date_time', 'Global_active_power']]\n",
    "df.sort_values('date_time', inplace=True, ascending=True)\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print('Number of rows and columns after removing missing values:', df.shape)\n",
    "print('The time series starts from: ', df['date_time'].min())\n",
    "print('The time series ends on: ', df['date_time'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "370f0919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2049280 entries, 0 to 2049279\n",
      "Data columns (total 2 columns):\n",
      " #   Column               Dtype         \n",
      "---  ------               -----         \n",
      " 0   date_time            datetime64[ns]\n",
      " 1   Global_active_power  float64       \n",
      "dtypes: datetime64[ns](1), float64(1)\n",
      "memory usage: 31.3 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ac3665a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_time</th>\n",
       "      <th>Global_active_power</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006-12-16 17:24:00</td>\n",
       "      <td>4.216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2006-12-16 17:25:00</td>\n",
       "      <td>5.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2006-12-16 17:26:00</td>\n",
       "      <td>5.374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2006-12-16 17:27:00</td>\n",
       "      <td>5.388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2006-12-16 17:28:00</td>\n",
       "      <td>3.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2006-12-16 17:29:00</td>\n",
       "      <td>3.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2006-12-16 17:30:00</td>\n",
       "      <td>3.702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2006-12-16 17:31:00</td>\n",
       "      <td>3.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2006-12-16 17:32:00</td>\n",
       "      <td>3.668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2006-12-16 17:33:00</td>\n",
       "      <td>3.662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date_time  Global_active_power\n",
       "0 2006-12-16 17:24:00                4.216\n",
       "1 2006-12-16 17:25:00                5.360\n",
       "2 2006-12-16 17:26:00                5.374\n",
       "3 2006-12-16 17:27:00                5.388\n",
       "4 2006-12-16 17:28:00                3.666\n",
       "5 2006-12-16 17:29:00                3.520\n",
       "6 2006-12-16 17:30:00                3.702\n",
       "7 2006-12-16 17:31:00                3.700\n",
       "8 2006-12-16 17:32:00                3.668\n",
       "9 2006-12-16 17:33:00                3.662"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3059c666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dates: 2010-12-05 00:00:00 to 2010-12-11 23:59:00\n",
      "Validation dates: 2010-11-21 00:00:00 to 2010-12-04 23:59:00\n",
      "Train dates: 2006-12-16 17:24:00 to 2010-11-20 23:59:00\n"
     ]
    }
   ],
   "source": [
    "# Split into training, validation and test datasets.\n",
    "# Since it's timeseries we should do it by date.\n",
    "test_cutoff_date = df['date_time'].max() - timedelta(days=7)\n",
    "val_cutoff_date = test_cutoff_date - timedelta(days=14)\n",
    "\n",
    "df_test = df[df['date_time'] > test_cutoff_date]\n",
    "df_val = df[(df['date_time'] > val_cutoff_date) & (df['date_time'] <= test_cutoff_date)]\n",
    "df_train = df[df['date_time'] <= val_cutoff_date]\n",
    "\n",
    "#check out the datasets\n",
    "print('Test dates: {} to {}'.format(df_test['date_time'].min(), df_test['date_time'].max()))\n",
    "print('Validation dates: {} to {}'.format(df_val['date_time'].min(), df_val['date_time'].max()))\n",
    "print('Train dates: {} to {}'.format(df_train['date_time'].min(), df_train['date_time'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb28f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal of the model:\n",
    "#  Predict Global_active_power at a specified time in the future.\n",
    "#   Eg. We want to predict how much Global_active_power will be ten minutes from now.\n",
    "#       We can use all the values from t-1, t-2, t-3, .... t-history_length to predict t+10\n",
    "def create_ts_files(dataset, \n",
    "                    start_index, \n",
    "                    end_index, \n",
    "                    history_length, \n",
    "                    step_size, \n",
    "                    target_step, \n",
    "                    num_rows_per_file, \n",
    "                    data_folder):\n",
    "    assert step_size > 0\n",
    "    assert start_index >= 0\n",
    "    \n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    \n",
    "    time_lags = sorted(range(target_step+1, target_step+history_length+1, step_size), reverse=True)\n",
    "    col_names = [f'x_lag{i}' for i in time_lags] + ['y']\n",
    "    start_index = start_index + history_length\n",
    "    if end_index is None:\n",
    "        end_index = len(dataset) - target_step\n",
    "    \n",
    "    rng = range(start_index, end_index)\n",
    "    num_rows = len(rng)\n",
    "    num_files = math.ceil(num_rows/num_rows_per_file)\n",
    "    \n",
    "    # for each file.\n",
    "    print(f'Creating {num_files} files.')\n",
    "    for i in range(num_files):\n",
    "        filename = f'{data_folder}/ts_file{i}.pkl'\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'{filename}')\n",
    "            \n",
    "        # get the start and end indices.\n",
    "        ind0 = i*num_rows_per_file\n",
    "        ind1 = min(ind0 + num_rows_per_file, end_index)\n",
    "        data_list = []\n",
    "        \n",
    "        # j in the current timestep. Will need j-n to j-1 for the history. And j + target_step for the target.\n",
    "        for j in range(ind0, ind1):\n",
    "            indices = range(j-1, j-history_length-1, -step_size)\n",
    "            data = dataset[sorted(indices) + [j+target_step]]\n",
    "            \n",
    "            # append data to the list.\n",
    "            data_list.append(data)\n",
    "\n",
    "        df_ts = pd.DataFrame(data=data_list, columns=col_names)\n",
    "        df_ts.to_pickle(filename)\n",
    "            \n",
    "    return len(col_names)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8511842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 158 files.\n",
      "ts_data/ts_file0.pkl\n",
      "ts_data/ts_file10.pkl\n",
      "ts_data/ts_file20.pkl\n",
      "ts_data/ts_file30.pkl\n",
      "ts_data/ts_file40.pkl\n",
      "ts_data/ts_file50.pkl\n",
      "ts_data/ts_file60.pkl\n",
      "ts_data/ts_file70.pkl\n",
      "ts_data/ts_file80.pkl\n",
      "ts_data/ts_file90.pkl\n",
      "ts_data/ts_file100.pkl\n",
      "ts_data/ts_file110.pkl\n",
      "ts_data/ts_file120.pkl\n",
      "ts_data/ts_file130.pkl\n",
      "ts_data/ts_file140.pkl\n",
      "ts_data/ts_file150.pkl\n",
      "Wall time: 10min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "global_active_power = df_train['Global_active_power'].values\n",
    "\n",
    "# Scaled to work with Neural networks.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "global_active_power_scaled = scaler.fit_transform(global_active_power.reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "history_length = 7*24*60  # The history length in minutes.\n",
    "step_size = 10  # The sampling rate of the history. Eg. If step_size = 1, then values from every minute will be in the history.\n",
    "                #                                       If step size = 10 then values every 10 minutes will be in the history.\n",
    "target_step = 10  # The time step in the future to predict. Eg. If target_step = 0, then predict the next timestep after the end of the history period.\n",
    "                  #                                             If target_step = 10 then predict 10 timesteps the next timestep (11 minutes after the end of history).\n",
    "\n",
    "# The csv creation returns the number of rows and number of features. We need these values below.\n",
    "num_timesteps = create_ts_files(global_active_power_scaled,\n",
    "                                start_index=0,\n",
    "                                end_index=None,\n",
    "                                history_length=history_length,\n",
    "                                step_size=step_size,\n",
    "                                target_step=target_step,\n",
    "                                num_rows_per_file=128*100,\n",
    "                                data_folder='ts_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a31fb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# So we can handle loading the data in chunks from the hard drive instead of having to load everything into memory.\n",
    "# \n",
    "# The reason we want to do this is so we can do custom processing on the data that we are feeding into the LSTM.\n",
    "# LSTM requires a certain shape and it is tricky to get it right.\n",
    "#\n",
    "class TimeSeriesLoader:\n",
    "    def __init__(self, ts_folder, filename_format):\n",
    "        self.ts_folder = ts_folder\n",
    "        \n",
    "        # find the number of files.\n",
    "        i = 0\n",
    "        file_found = True\n",
    "        while file_found:\n",
    "            filename = self.ts_folder + '/' + filename_format.format(i)\n",
    "            file_found = os.path.exists(filename)\n",
    "            if file_found:\n",
    "                i += 1\n",
    "                \n",
    "        self.num_files = i\n",
    "        self.files_indices = np.arange(self.num_files)\n",
    "        self.shuffle_chunks()\n",
    "        \n",
    "    def num_chunks(self):\n",
    "        return self.num_files\n",
    "    \n",
    "    def get_chunk(self, idx):\n",
    "        assert (idx >= 0) and (idx < self.num_files)\n",
    "        \n",
    "        ind = self.files_indices[idx]\n",
    "        filename = self.ts_folder + '/' + filename_format.format(ind)\n",
    "        df_ts = pd.read_pickle(filename)\n",
    "        num_records = len(df_ts.index)\n",
    "        \n",
    "        features = df_ts.drop('y', axis=1).values\n",
    "        target = df_ts['y'].values\n",
    "        \n",
    "        # reshape for input into LSTM. Batch major format.\n",
    "        features_batchmajor = np.array(features).reshape(num_records, -1, 1)\n",
    "        return features_batchmajor, target\n",
    "    \n",
    "    # this shuffles the order the chunks will be outputted from get_chunk.\n",
    "    def shuffle_chunks(self):\n",
    "        np.random.shuffle(self.files_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06a1950a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_folder = 'ts_data'\n",
    "filename_format = 'ts_file{}.pkl'\n",
    "tss = TimeSeriesLoader(ts_folder, filename_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe005004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Keras model.\n",
    "# Use hyperparameter optimization if you have the time.\n",
    "\n",
    "ts_inputs = tf.keras.Input(shape=(num_timesteps, 1))\n",
    "\n",
    "# units=10 -> The cell and hidden states will be of dimension 10.\n",
    "#             The number of parameters that need to be trained = 4*units*(units+2)\n",
    "x = layers.LSTM(units=10)(ts_inputs)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(1, activation='linear')(x)\n",
    "model = tf.keras.Model(inputs=ts_inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0369715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the training configuration.\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.01),\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f7c288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1008, 1)]         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 10)                480       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 491\n",
      "Trainable params: 491\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07c80efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch #0\n",
      "100/100 [==============================] - 17s 163ms/step - loss: 0.0130 - mse: 0.0130\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0073 - mse: 0.0073\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0085 - mse: 0.0085\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0086 - mse: 0.00863s - loss: \n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0115 - mse: 0.0115\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0088 - mse: 0.0088\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0053 - mse: 0.0053\n",
      "100/100 [==============================] - 16s 164ms/step - loss: 0.0107 - mse: 0.0107\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0110 - mse: 0.0110\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0057 - mse: 0.0057\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0136 - mse: 0.0136\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0105 - mse: 0.0105\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0054 - mse: 0.0054\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0057 - mse: 0.0057\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0067 - mse: 0.0067\n",
      "100/100 [==============================] - 16s 164ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0057 - mse: 0.0057\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0066 - mse: 0.0066\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0055 - mse: 0.0055\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0073 - mse: 0.0073\n",
      "100/100 [==============================] - 16s 158ms/step - loss: 5.0281e-04 - mse: 5.0281e-04\n",
      "100/100 [==============================] - 16s 158ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0051 - mse: 0.0051\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0065 - mse: 0.0065\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0066 - mse: 0.0066\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0071 - mse: 0.0071\n",
      "100/100 [==============================] - 16s 158ms/step - loss: 0.0077 - mse: 0.0077\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0059 - mse: 0.0059\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0032 - mse: 0.0032\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0053 - mse: 0.0053\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0072 - mse: 0.0072\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0058 - mse: 0.0058\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0036 - mse: 0.00361s - loss: 0.0036 - ms\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0020 - mse: 0.0020\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0069 - mse: 0.0069\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0054 - mse: 0.0054\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0065 - mse: 0.0065\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0033 - mse: 0.0033\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0057 - mse: 0.0057\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0033 - mse: 0.0033\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0073 - mse: 0.0073\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0051 - mse: 0.0051\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0059 - mse: 0.0059\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0058 - mse: 0.0058\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0023 - mse: 0.0023\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0032 - mse: 0.0032\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0052 - mse: 0.0052\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0061 - mse: 0.0061\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0031 - mse: 0.0031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0033 - mse: 0.0033\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0030 - mse: 0.0030\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0045 - mse: 0.0045\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0017 - mse: 0.0017\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0053 - mse: 0.0053\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0054 - mse: 0.0054\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0020 - mse: 0.0020\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0042 - mse: 0.0042\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0046 - mse: 0.0046\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0061 - mse: 0.0061\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0019 - mse: 0.0019\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0034 - mse: 0.0034\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0043 - mse: 0.0043\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0038 - mse: 0.0038\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 16s 164ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0040 - mse: 0.0040\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0036 - mse: 0.0036\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0030 - mse: 0.0030\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0012 - mse: 0.0012\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0033 - mse: 0.0033\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0015 - mse: 0.0015\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0051 - mse: 0.0051\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0033 - mse: 0.0033\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0019 - mse: 0.0019\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0049 - mse: 0.0049\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0060 - mse: 0.0060\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0037 - mse: 0.0037\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0054 - mse: 0.0054\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0058 - mse: 0.0058\n",
      "100/100 [==============================] - 16s 159ms/step - loss: 0.0057 - mse: 0.0057\n",
      "100/100 [==============================] - 16s 157ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0044 - mse: 0.0044\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0039 - mse: 0.0039\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0048 - mse: 0.0048\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.0041 - mse: 0.0041\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0050 - mse: 0.0050\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0047 - mse: 0.0047\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0074 - mse: 0.0074\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0051 - mse: 0.0051\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.0027 - mse: 0.0027\n",
      "100/100 [==============================] - 16s 162ms/step - loss: 0.0035 - mse: 0.0035\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.0031 - mse: 0.0031\n",
      "Wall time: 43min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# train in batch sizes of 128.\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 1\n",
    "NUM_CHUNKS = tss.num_chunks()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print('epoch #{}'.format(epoch))\n",
    "    for i in range(NUM_CHUNKS):\n",
    "        X, y = tss.get_chunk(i)\n",
    "        \n",
    "        # model.fit does train the model incrementally. ie. Can call multiple times in batches.\n",
    "        # https://github.com/keras-team/keras/issues/4446\n",
    "        model.fit(x=X, y=y, batch_size=BATCH_SIZE)\n",
    "        \n",
    "    # shuffle the chunks so they're not in the same order next time around.\n",
    "    tss.shuffle_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12999d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating 1 files.\n",
      "ts_val_data/ts_file0.pkl\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the validation set.\n",
    "#\n",
    "# Create the validation CSV like we did before with the training.\n",
    "global_active_power_val = df_val['Global_active_power'].values\n",
    "global_active_power_val_scaled = scaler.transform(global_active_power_val.reshape(-1, 1)).reshape(-1, )\n",
    "\n",
    "history_length = 7*24*60  # The history length in minutes.\n",
    "step_size = 10  # The sampling rate of the history. Eg. If step_size = 1, then values from every minute will be in the history.\n",
    "                #                                       If step size = 10 then values every 10 minutes will be in the history.\n",
    "target_step = 10  # The time step in the future to predict. Eg. If target_step = 0, then predict the next timestep after the end of the history period.\n",
    "                  #                                             If target_step = 10 then predict 10 timesteps the next timestep (11 minutes after the end of history).\n",
    "\n",
    "# The csv creation returns the number of rows and number of features. We need these values below.\n",
    "num_timesteps = create_ts_files(global_active_power_val_scaled,\n",
    "                                start_index=0,\n",
    "                                end_index=None,\n",
    "                                history_length=history_length,\n",
    "                                step_size=step_size,\n",
    "                                target_step=target_step,\n",
    "                                num_rows_per_file=128*100,\n",
    "                                data_folder='ts_val_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dac700db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation mean squared error: 0.40967758842160457\n",
      "validation baseline mean squared error: 0.428345914375\n"
     ]
    }
   ],
   "source": [
    "# If we assume that the validation dataset can fit into memory we can do this.\n",
    "df_val_ts = pd.read_pickle('ts_val_data/ts_file0.pkl')\n",
    "\n",
    "\n",
    "features = df_val_ts.drop('y', axis=1).values\n",
    "features_arr = np.array(features)\n",
    "\n",
    "# reshape for input into LSTM. Batch major format.\n",
    "num_records = len(df_val_ts.index)\n",
    "features_batchmajor = features_arr.reshape(num_records, -1, 1)\n",
    "\n",
    "\n",
    "y_pred = model.predict(features_batchmajor).reshape(-1, )\n",
    "y_pred = scaler.inverse_transform(y_pred.reshape(-1, 1)).reshape(-1 ,)\n",
    "\n",
    "y_act = df_val_ts['y'].values\n",
    "y_act = scaler.inverse_transform(y_act.reshape(-1, 1)).reshape(-1 ,)\n",
    "\n",
    "print('validation mean squared error: {}'.format(mean_squared_error(y_act, y_pred)))\n",
    "\n",
    "#baseline\n",
    "y_pred_baseline = df_val_ts['x_lag11'].values\n",
    "y_pred_baseline = scaler.inverse_transform(y_pred_baseline.reshape(-1, 1)).reshape(-1 ,)\n",
    "\n",
    "print('validation baseline mean squared error: {}'.format(mean_squared_error(y_act, y_pred_baseline)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1d4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
