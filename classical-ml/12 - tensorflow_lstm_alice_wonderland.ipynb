{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../datasets/wonderland.txt\"\n",
    "raw_text = open(filename,\"r\",encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144583\n",
      "Total Vocab:  50\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144483\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape = (X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation = 'softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor = 'loss' , verbose=1, save_best_only = True, mode = 'min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000276C6BCF948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x00000276C6BCF948> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 3.0207\n",
      "Epoch 1: loss improved from inf to 3.02070, saving model to weights-improvement-01-3.0207.hdf5\n",
      "1129/1129 [==============================] - 214s 188ms/step - loss: 3.0207\n",
      "Epoch 2/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.8134\n",
      "Epoch 2: loss improved from 3.02070 to 2.81344, saving model to weights-improvement-02-2.8134.hdf5\n",
      "1129/1129 [==============================] - 211s 187ms/step - loss: 2.8134\n",
      "Epoch 3/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.7107\n",
      "Epoch 3: loss improved from 2.81344 to 2.71074, saving model to weights-improvement-03-2.7107.hdf5\n",
      "1129/1129 [==============================] - 217s 192ms/step - loss: 2.7107\n",
      "Epoch 4/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.6394\n",
      "Epoch 4: loss improved from 2.71074 to 2.63944, saving model to weights-improvement-04-2.6394.hdf5\n",
      "1129/1129 [==============================] - 212s 188ms/step - loss: 2.6394\n",
      "Epoch 5/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.5755\n",
      "Epoch 5: loss improved from 2.63944 to 2.57551, saving model to weights-improvement-05-2.5755.hdf5\n",
      "1129/1129 [==============================] - 209s 185ms/step - loss: 2.5755\n",
      "Epoch 6/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.5162\n",
      "Epoch 6: loss improved from 2.57551 to 2.51618, saving model to weights-improvement-06-2.5162.hdf5\n",
      "1129/1129 [==============================] - 211s 187ms/step - loss: 2.5162\n",
      "Epoch 7/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.4627\n",
      "Epoch 7: loss improved from 2.51618 to 2.46273, saving model to weights-improvement-07-2.4627.hdf5\n",
      "1129/1129 [==============================] - 205s 182ms/step - loss: 2.4627\n",
      "Epoch 8/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.4112\n",
      "Epoch 8: loss improved from 2.46273 to 2.41120, saving model to weights-improvement-08-2.4112.hdf5\n",
      "1129/1129 [==============================] - 203s 180ms/step - loss: 2.4112\n",
      "Epoch 9/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.3650\n",
      "Epoch 9: loss improved from 2.41120 to 2.36504, saving model to weights-improvement-09-2.3650.hdf5\n",
      "1129/1129 [==============================] - 202s 179ms/step - loss: 2.3650\n",
      "Epoch 10/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.3217\n",
      "Epoch 10: loss improved from 2.36504 to 2.32168, saving model to weights-improvement-10-2.3217.hdf5\n",
      "1129/1129 [==============================] - 202s 179ms/step - loss: 2.3217\n",
      "Epoch 11/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.2789\n",
      "Epoch 11: loss improved from 2.32168 to 2.27894, saving model to weights-improvement-11-2.2789.hdf5\n",
      "1129/1129 [==============================] - 203s 179ms/step - loss: 2.2789\n",
      "Epoch 12/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.2369\n",
      "Epoch 12: loss improved from 2.27894 to 2.23692, saving model to weights-improvement-12-2.2369.hdf5\n",
      "1129/1129 [==============================] - 203s 180ms/step - loss: 2.2369\n",
      "Epoch 13/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.2003\n",
      "Epoch 13: loss improved from 2.23692 to 2.20032, saving model to weights-improvement-13-2.2003.hdf5\n",
      "1129/1129 [==============================] - 203s 180ms/step - loss: 2.2003\n",
      "Epoch 14/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.1628\n",
      "Epoch 14: loss improved from 2.20032 to 2.16279, saving model to weights-improvement-14-2.1628.hdf5\n",
      "1129/1129 [==============================] - 207s 183ms/step - loss: 2.1628\n",
      "Epoch 15/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.1275\n",
      "Epoch 15: loss improved from 2.16279 to 2.12745, saving model to weights-improvement-15-2.1275.hdf5\n",
      "1129/1129 [==============================] - 209s 185ms/step - loss: 2.1275\n",
      "Epoch 16/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0918\n",
      "Epoch 16: loss improved from 2.12745 to 2.09181, saving model to weights-improvement-16-2.0918.hdf5\n",
      "1129/1129 [==============================] - 208s 184ms/step - loss: 2.0918\n",
      "Epoch 17/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0616\n",
      "Epoch 17: loss improved from 2.09181 to 2.06157, saving model to weights-improvement-17-2.0616.hdf5\n",
      "1129/1129 [==============================] - 208s 184ms/step - loss: 2.0616\n",
      "Epoch 18/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0315\n",
      "Epoch 18: loss improved from 2.06157 to 2.03152, saving model to weights-improvement-18-2.0315.hdf5\n",
      "1129/1129 [==============================] - 208s 184ms/step - loss: 2.0315\n",
      "Epoch 19/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0040\n",
      "Epoch 19: loss improved from 2.03152 to 2.00402, saving model to weights-improvement-19-2.0040.hdf5\n",
      "1129/1129 [==============================] - 208s 185ms/step - loss: 2.0040\n",
      "Epoch 20/20\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.9751\n",
      "Epoch 20: loss improved from 2.00402 to 1.97506, saving model to weights-improvement-20-1.9751.hdf5\n",
      "1129/1129 [==============================] - 213s 188ms/step - loss: 1.9751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x276c6fa8048>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs = 20, batch_size = 128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-20-1.9751.hdf5\"\n",
    "\n",
    "model.load_weights(filename)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "e\"r\",\"\n",
      "\"y\"o\"u\" \"k\"n\"o\"w\".\"”\"\n",
      "\"\n",
      "\"a\"l\"i\"c\"e\" \"h\"a\"d\" \"b\"e\"e\"n\" \"l\"o\"o\"k\"i\"n\"g\" \"o\"v\"e\"r\" \"h\"i\"s\" \"s\"h\"o\"u\"l\"d\"e\"r\" \"w\"i\"t\"h\" \"s\"o\"m\"e\" \"c\"u\"r\"i\"o\"s\"i\"t\"y\".\" \"“\"w\"h\"a\"t\" \"a\"\n",
      "\"f\"u\"n\"n\"y\" \"w\"a\"t\"c\"h\"!\"”\"  \"\n",
      "\n",
      "“ho ay all ”hu an _nd i can ”ou ” said the mock turtle in a lonat oone tone, “io was an all to tee she wait wo be io!”\n",
      "\n",
      "“ho woat at all ”arl to t aadad a gad aeiin, in aelned ii the some of the soids so tee  the dould nete the was oo the was of the tood wfth the court, \n",
      "“ie you dnn’t know what io ”hu an i connd _ tirn iir ”oul sound ”ou cinne!” the manch hare sniepeyg thete the was soine to tay the wante tf tee soohe the was oo the taateng and toeneng to tee it tar to ced aod toeneng to taek the was oo the thoh the was oo the taaten \n",
      "the was oo toee an tole tioe the was oo toe tint hir haad to tee whut hnr tored ana toeken the was oo toe toone. \n",
      "“whet _il _aout a lenelen,” said the caterpillar.\n",
      "\n",
      "“ie you taen to dl ” said the mock turtle in a lonat oone tone, “io was an all to tee shan i can’t be aored teth the sooes ”ou’ eeve r and toen i cru’t tein to tee what io whuh the waine of the sood. \n",
      "“h whnh t ean _elin,” said the caterpillar.\n",
      "\n",
      "“ie you taen to dl ” said the mock turtle in a l\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\".join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    \n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "    \n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
